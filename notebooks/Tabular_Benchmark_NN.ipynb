{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hpobench.benchmarks.ml.tabular_benchmark import TabularBenchmark\n",
    "from ConfigSpace.hyperparameters import OrdinalHyperparameter\n",
    "from ConfigSpace.configuration_space import ConfigurationSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = TabularBenchmark('nn', 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_ordinal(value: float, hyperparameter_type: OrdinalHyperparameter):\n",
    "    valid_values = np.array(hyperparameter_type.sequence)\n",
    "    nearest = np.argmin((valid_values - value)**2).item()\n",
    "    order = hyperparameter_type.get_seq_order()\n",
    "    return hyperparameter_type.get_value(order[nearest])\n",
    "\n",
    "def round_to_valid_config(values: dict, space: ConfigurationSpace):\n",
    "    return {hyperparameter.name:find_nearest_ordinal(values[hyperparameter.name], hyperparameter) for hyperparameter in space.get_hyperparameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_config = {\"alpha\": 0.001, \"batch_size\": 32, \"depth\": 2.0, \"learning_rate_init\": 0.001, \"width\": 64}\n",
    "valid_config = round_to_valid_config({\"alpha\": 0.001, \"batch_size\": 32, \"depth\": 2.0, \"learning_rate_init\": 0.001, \"width\": 64}, benchmark.configuration_space)\n",
    "result = benchmark.objective_function(valid_config)\n",
    "(result['function_value'], result['cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = benchmark.table[[\"alpha\",\"batch_size\",\"depth\",\"learning_rate_init\",\"width\",\"iter\"]]\n",
    "results = pd.json_normalize(benchmark.table[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation of benchmark into an MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_hidden_units = 128\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_y = nn.Sequential(OrderedDict([\n",
    "    ('W1b', nn.Linear(6, num_hidden_units, bias=True)),\n",
    "    ('activations', nn.ELU()),\n",
    "    ('W2b', nn.Linear(num_hidden_units, 1, bias=True)),\n",
    "]))\n",
    "\n",
    "model_log_c = nn.Sequential(OrderedDict([\n",
    "    ('W1b', nn.Linear(6, num_hidden_units, bias=True)),\n",
    "    ('activations', nn.ELU()),\n",
    "    ('W2b', nn.Linear(num_hidden_units, 1, bias=True)),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(inputs.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(results[\"function_value\"], dtype=torch.float32).unsqueeze(1)\n",
    "log_c_tensor = torch.log(torch.tensor(results[\"cost\"], dtype=torch.float32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_tensor.mean(), y_tensor.std(), y_tensor.min(), y_tensor.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(log_c_tensor.mean(), log_c_tensor.std(), log_c_tensor.min(), log_c_tensor.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer_y = torch.optim.Adam(model_y.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_log_c = torch.optim.Adam(model_log_c.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_y = TensorDataset(X_tensor,y_tensor)\n",
    "dataset_log_c = TensorDataset(X_tensor,log_c_tensor)\n",
    "dataloader_y = DataLoader(dataset_y, batch_size=batch_size, shuffle=True)\n",
    "dataloader_log_c = DataLoader(dataset_log_c, batch_size=batch_size, shuffle=True)\n",
    "size = X_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch, (X, y) in (pbar := tqdm(enumerate(dataloader_y))):\n",
    "        pred = model_y(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer_y.step()\n",
    "        optimizer_y.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            pbar.set_description(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y_tensor.min(), model_y(X_tensor).min()), (y_tensor.max(), model_y(X_tensor).max()), loss_fn(model_y(X_tensor), y_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch, (X, log_c) in (pbar := tqdm(enumerate(dataloader_log_c))):\n",
    "        pred = model_log_c(X)\n",
    "        loss = loss_fn(pred, log_c)\n",
    "        loss.backward()\n",
    "        optimizer_log_c.step()\n",
    "        optimizer_log_c.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            pbar.set_description(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((log_c_tensor.min(), model_log_c(X_tensor).min()), (log_c_tensor.max(), model_log_c(X_tensor).max()), loss_fn(model_log_c(X_tensor), log_c_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.save(model_y.state_dict(), \"distilled_nn_y_model.pickle\")\n",
    "    torch.save({\"min\": model_y(X_tensor).min(), \"max\": model_y(X_tensor).max()}, \"distilled_nn_y_model_min_max.pickle\")\n",
    "    torch.save(model_log_c.state_dict(), \"distilled_nn_log_c_model.pickle\")\n",
    "    torch.save({\"min\": model_log_c(X_tensor).min(), \"max\": model_log_c(X_tensor).max()}, \"distilled_nn_log_c_model_min_max.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num_hidden_units = 128\n",
    "\n",
    "test_model_y = nn.Sequential(OrderedDict([\n",
    "    ('W1b', nn.Linear(6, test_num_hidden_units, bias=True)),\n",
    "    ('activations', nn.ELU()),\n",
    "    ('W2b', nn.Linear(test_num_hidden_units, 1, bias=True)),\n",
    "]))\n",
    "\n",
    "test_model_log_c = nn.Sequential(OrderedDict([\n",
    "    ('W1b', nn.Linear(6, test_num_hidden_units, bias=True)),\n",
    "    ('activations', nn.ELU()),\n",
    "    ('W2b', nn.Linear(test_num_hidden_units, 1, bias=True)),\n",
    "]))\n",
    "\n",
    "test_model_y.load_state_dict(torch.load(\"distilled_nn_y_model.pickle\"))\n",
    "test_model_log_c.load_state_dict(torch.load(\"distilled_nn_log_c_model.pickle\"))\n",
    "(torch.load(\"distilled_nn_y_model_min_max.pickle\"), torch.load(\"distilled_nn_log_c_model_min_max.pickle\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
