{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "def load_bayesian_regret_fixed_cost_runs(acq, dim, kernel, lengthscale):\n",
    "    runs = api.runs(path=\"ziv-scully-group/PandoraBayesOPT\", filters={\n",
    "        \"sweep\": \"36h918l7\",\n",
    "        \"config.amplitude\": 1,\n",
    "        \"config.dim\": dim,\n",
    "        \"config.kernel\": kernel,\n",
    "        \"config.lengthscale\": lengthscale,  \n",
    "        \"config.policy\": acq})\n",
    "    \n",
    "    configs_and_metrics = []\n",
    "    for run in tqdm(runs):\n",
    "        metric_keys = [\"cumulative cost\",\"best observed\"]\n",
    "        history = run.scan_history(keys = metric_keys, page_size=1_000_000_000)\n",
    "        metrics = {k: [d[k] for d in history] for k in metric_keys}\n",
    "        summary_metric_keys = [\"global optimum value\"]\n",
    "        summary_history = run.scan_history(keys = summary_metric_keys, page_size=1_000_000_000)\n",
    "        metrics.update({k: [d[k] for d in summary_history] for k in summary_metric_keys})\n",
    "        configs_and_metrics.append((run.config, metrics))\n",
    "\n",
    "    return configs_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_functions = {\n",
    "    'UpperConfidenceBound': 'UpperConfidenceBound',\n",
    "    }\n",
    "dimensions = [4, 8, 16, 32]\n",
    "kernels = [\"Matern32\", \"Matern52\", \"RBF\"]\n",
    "lengthscales = [1.0, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.20it/s]\n",
      "100%|██████████| 16/16 [00:13<00:00,  1.16it/s]\n",
      "100%|██████████| 16/16 [00:13<00:00,  1.15it/s]\n",
      "100%|██████████| 16/16 [00:11<00:00,  1.36it/s]\n",
      "100%|██████████| 16/16 [00:16<00:00,  1.01s/it]\n",
      "100%|██████████| 16/16 [00:20<00:00,  1.30s/it]\n",
      "100%|██████████| 16/16 [00:20<00:00,  1.30s/it]\n",
      "100%|██████████| 16/16 [00:19<00:00,  1.25s/it]\n",
      "100%|██████████| 16/16 [00:19<00:00,  1.19s/it]\n",
      "100%|██████████| 16/16 [00:20<00:00,  1.31s/it]\n",
      "100%|██████████| 16/16 [00:20<00:00,  1.30s/it]\n",
      "100%|██████████| 16/16 [00:19<00:00,  1.23s/it]\n",
      "100%|██████████| 16/16 [00:20<00:00,  1.25s/it]\n",
      "100%|██████████| 16/16 [00:22<00:00,  1.43s/it]\n",
      "100%|██████████| 16/16 [00:18<00:00,  1.18s/it]\n",
      "100%|██████████| 16/16 [00:21<00:00,  1.37s/it]\n",
      " 88%|████████▊ | 14/16 [00:18<00:02,  1.39s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "100%|██████████| 16/16 [14:54<00:00, 55.92s/it] \n",
      "100%|██████████| 16/16 [00:32<00:00,  2.02s/it]\n",
      " 31%|███▏      | 5/16 [00:06<00:13,  1.24s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n",
      "100%|██████████| 16/16 [26:24<00:00, 99.00s/it]  \n",
      "100%|██████████| 16/16 [00:21<00:00,  1.36s/it]\n",
      "100%|██████████| 16/16 [00:18<00:00,  1.18s/it]\n",
      "100%|██████████| 16/16 [00:19<00:00,  1.24s/it]\n",
      "100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
      "100%|██████████| 16/16 [00:19<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "grouped_runs = {(a,d,k,l): load_bayesian_regret_fixed_cost_runs(a,d,k,l) for a in acquisition_functions.keys() for d in (dimensions) for k in (kernels) for l in (lengthscales)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpperConfidenceBound 4 Matern32 1.0 (41, 16)\n",
      "UpperConfidenceBound 4 Matern32 0.1 (101, 16)\n",
      "UpperConfidenceBound 4 Matern52 1.0 (41, 16)\n",
      "UpperConfidenceBound 4 Matern52 0.1 (101, 16)\n",
      "UpperConfidenceBound 4 RBF 1.0 (21, 16)\n",
      "UpperConfidenceBound 4 RBF 0.1 (81, 16)\n",
      "UpperConfidenceBound 8 Matern32 1.0 (81, 16)\n",
      "UpperConfidenceBound 8 Matern32 0.1 (201, 16)\n",
      "UpperConfidenceBound 8 Matern52 1.0 (81, 16)\n",
      "UpperConfidenceBound 8 Matern52 0.1 (201, 16)\n",
      "UpperConfidenceBound 8 RBF 1.0 (41, 16)\n",
      "UpperConfidenceBound 8 RBF 0.1 (161, 16)\n",
      "UpperConfidenceBound 16 Matern32 1.0 (161, 16)\n",
      "UpperConfidenceBound 16 Matern32 0.1 (401, 16)\n",
      "UpperConfidenceBound 16 Matern52 1.0 (161, 16)\n",
      "UpperConfidenceBound 16 Matern52 0.1 (401, 16)\n",
      "UpperConfidenceBound 16 RBF 1.0 (81, 16)\n",
      "UpperConfidenceBound 16 RBF 0.1 (321, 16)\n",
      "UpperConfidenceBound 32 Matern32 1.0 (321, 16)\n",
      "UpperConfidenceBound 32 Matern32 0.1 (801, 16)\n",
      "UpperConfidenceBound 32 Matern52 1.0 (321, 16)\n",
      "UpperConfidenceBound 32 Matern52 0.1 (801, 16)\n",
      "UpperConfidenceBound 32 RBF 1.0 (161, 16)\n",
      "UpperConfidenceBound 32 RBF 0.1 (641, 16)\n"
     ]
    }
   ],
   "source": [
    "for a in acquisition_functions.keys():\n",
    "    for d in dimensions:\n",
    "        for k in kernels:\n",
    "            for l in lengthscales:\n",
    "                config_and_metrics_per_seed = grouped_runs[a,d,k,l]\n",
    "        \n",
    "                cumulative_cost_per_seed = np.array([m['cumulative cost'] for (_,m) in config_and_metrics_per_seed if len(m['cumulative cost'])>0 and len(m['best observed'])>0]).T\n",
    "                best_observed_per_seed = np.array([m['best observed'] for (_,m) in config_and_metrics_per_seed if len(m['cumulative cost'])>0 and len(m['best observed'])>0]).T\n",
    "                # Handling potential empty arrays\n",
    "                if cumulative_cost_per_seed.size == 0 or best_observed_per_seed.size == 0:\n",
    "                    continue  # Skip this iteration if there's no data\n",
    "                global_optimum_per_seed = np.array([m['global optimum value'][0] for (_,m) in config_and_metrics_per_seed if len(m['cumulative cost'])>0 and len(m['best observed'])>0])\n",
    "        \n",
    "                regret_per_seed = global_optimum_per_seed - best_observed_per_seed\n",
    "                print(a, d, k, l, regret_per_seed.shape)\n",
    "        \n",
    "                regret_25 = np.quantile(regret_per_seed, 0.25, axis=1)\n",
    "                regret_50 = np.quantile(regret_per_seed, 0.5, axis=1)\n",
    "                regret_75 = np.quantile(regret_per_seed, 0.75, axis=1)\n",
    "        \n",
    "                output = np.stack((cumulative_cost_per_seed.mean(axis=1), regret_25, regret_50, regret_75),axis=-1)\n",
    "        \n",
    "                np.savetxt(f\"results/fixed_amplitude/BayesianRegret_FixedCost_d{d}_{k}_ls{l}_{acquisition_functions[a]}.csv\", output, header=\"cc, q25, q50, q75\", delimiter=', ', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
